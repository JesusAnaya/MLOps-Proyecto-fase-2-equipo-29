{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/damian/mlops-proyecto-fase-1-equipo-29/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Etapa 2.** Procesamiento y transformación de datos\n",
    "\n",
    "**Objetivos:**\n",
    "\n",
    "* Realizar transformaciones necesarias para preparar los datos para el análisis y modelado.\n",
    "\n",
    "* Realizar análisis exploratorio de datos utilizando visualizaciones y estadísticas descriptivas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "Con el análisis exploratorio que realizamos en el paso 1, descubrimos que tenemos variables categóricas, numéricas y binarias, y somos concientes de que nos enfrentamos a un problema de Outliers en varias de las variables. \n",
    "\n",
    "Con ese conocimiento, éste punto pretende realizar la generación del pipeline para la transformación de los datos, en preparación para el entrenamiento del modelo.\n",
    "\n",
    "* **Datos categóricos:** Aplicación de One Hot Enconder\n",
    "* **Datos numéricos:** Estandarización de datos\n",
    "* **Datos binarios:** Transformación de las categorías a 1 y 0.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paqueterías usadas en este archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.metrics import fbeta_score, make_scorer, precision_recall_curve, PrecisionRecallDisplay, RocCurveDisplay, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de la data set limpio, obtenido en el código 01: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lectura exitosa del archivo de datos crudos.\n",
      "Filas leídas: 992\n",
      "\n",
      "Primeras 5 filas:\n",
      "   laufkont  laufzeit  moral  verw   hoehe  sparkont  beszeit  rate  famges  \\\n",
      "0       1.0      18.0    4.0   2.0  1049.0       1.0      2.0   4.0     2.0   \n",
      "1       1.0       9.0    4.0   0.0  2799.0       1.0      3.0   2.0     3.0   \n",
      "2       2.0      12.0    2.0   9.0   841.0       2.0      4.0   2.0     2.0   \n",
      "3       1.0      12.0    4.0   0.0  2122.0       1.0      3.0   3.0     3.0   \n",
      "4       1.0      12.0    4.0   0.0  2171.0       1.0      3.0   4.0     3.0   \n",
      "\n",
      "   buerge  ...  verm  alter  weitkred  wohn  bishkred  beruf  pers  telef  \\\n",
      "0     1.0  ...   2.0   21.0       3.0   1.0       1.0    3.0   2.0    1.0   \n",
      "1     1.0  ...   1.0   36.0       3.0   1.0       2.0    3.0   1.0    1.0   \n",
      "2     1.0  ...   1.0   23.0       3.0   1.0       1.0    2.0   2.0    1.0   \n",
      "3     1.0  ...   1.0   39.0       3.0   1.0       2.0    2.0   1.0    1.0   \n",
      "4     NaN  ...   2.0   38.0       1.0   2.0       2.0    2.0   NaN    1.0   \n",
      "\n",
      "   gastarb  kredit  \n",
      "0      2.0     1.0  \n",
      "1      2.0     1.0  \n",
      "2      2.0     1.0  \n",
      "3      1.0     1.0  \n",
      "4      1.0     1.0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "path_clean = \"../../data/processed/data_clean.csv\"\n",
    "\n",
    "try:\n",
    "    # Leer el CSV usando la ruta relativa\n",
    "    data_clean = pd.read_csv(path_clean)\n",
    "\n",
    "    print(\"Lectura exitosa del archivo de datos crudos.\")\n",
    "    print(f\"Filas leídas: {len(data_clean)}\")\n",
    "    print(\"\\nPrimeras 5 filas:\")\n",
    "    print(data_clean.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: No se pudo encontrar el archivo en la ruta: {path_clean}\")\n",
    "    print(\"Asegúrate de que la ruta relativa sea correcta desde la ubicación de este notebook.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error inesperado durante la lectura: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Definición de Nombres de Columnas ---\n",
    "\n",
    "# Variables Numéricas\n",
    "numericas_pipe_nombres = ['laufzeit', 'hoehe', 'alter']\n",
    "\n",
    "# Variables Categóricas-Nominales (que requieren One-Hot Encoding)\n",
    "nominales_pipe_nombres = [\n",
    "    'laufkont', # (status - categorical)\n",
    "    'moral',    # (credit_history - categorical)\n",
    "    'verw',     # (purpose - categorical)\n",
    "    'sparkont', # (savings - categorical)\n",
    "    'famges',   # (personal_status_sex - categorical)\n",
    "    'buerge',   # (other_debtors - categorical)\n",
    "    'weitkred', # (other_installment_plans - categorical)\n",
    "    'wohn',     # (housing - categorical)\n",
    "    'pers',     # (people_liable - binary)\n",
    "    'telef',    # (telephone - binary)\n",
    "    'gastarb'   # (foreign_worker - binary)\n",
    "]\n",
    "\n",
    "# Variables Categóricas-Ordinales (que requieren Ordinal Encoding)\n",
    "ordinales_pipe_nombres = [\n",
    "    'beszeit',  # (employment_duration - ordinal)\n",
    "    'rate',     # (installment_rate - ordinal)\n",
    "    'wohnzeit', # (present_residence - ordinal)\n",
    "    'verm',     # (property - ordinal)\n",
    "    'bishkred', # (number_credits - ordinal)\n",
    "    'beruf'     # (job - ordinal)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PIPELINE DE TRANSFORMACIÓN DE LOS DATOS (ÉSTA VERSIÓN SE PUEDE APLICAR SI DECIDIMOS ELIMINAR NULOS)\n",
    "\n",
    "# Variables numéricas:\n",
    "numericas_pipe = Pipeline(steps = [('impMediana', SimpleImputer(strategy='median')),\n",
    "                                 ('escalaNum', MinMaxScaler(feature_range=(1,2)))])\n",
    "\n",
    "# Variables categóricas-Nominales:\n",
    "nominales_pipe = Pipeline(steps = [('impModa', SimpleImputer(strategy='most_frequent')),\n",
    "                             ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore'))])\n",
    "\n",
    "# Variables categóricas-ordinales:\n",
    "ordinales_pipe = Pipeline(steps = [('impOrd', SimpleImputer(strategy='most_frequent')),\n",
    "                                ('ordtrasnf', OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1))])\n",
    "\n",
    "# Conjuntas las transformaciones de todo tipo de variable y\n",
    "# deja sin procesar aquellas que hayas decidido no transformar:\n",
    "\n",
    "columnasTransformer = ColumnTransformer(transformers = [('numpipe', numericas_pipe, numericas_pipe_nombres),\n",
    "                                                        ('nominals', nominales_pipe, nominales_pipe_nombres),\n",
    "                                                        ('ordinales', ordinales_pipe, ordinales_pipe_nombres)],\n",
    "                                        remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PIPELINE DE TRANSFORMACIÓN DE LOS DATOS (ÉSTA VERSIÓN SE PUEDE APLICAR SI DECIDIMOS IMPUTAR NULOS)\n",
    "\n",
    "# --- 2. Definición de Pipelines Individuales (con Imputación) ---\n",
    "\n",
    "# Pipeline para Variables Numéricas: Imputación con Mediana + Escalado MinMax\n",
    "numericas_pipe = Pipeline(steps = [\n",
    "    ('impMediana', SimpleImputer(strategy='median')),\n",
    "    ('escalaNum', MinMaxScaler(feature_range=(1,2)))\n",
    "])\n",
    "\n",
    "# Pipeline para Variables Nominales: Imputación con Moda + One-Hot Encoding\n",
    "nominales_pipe = Pipeline(steps = [\n",
    "    ('impModa', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Pipeline para Variables Ordinales: Imputación con Moda + Ordinal Encoding\n",
    "ordinales_pipe = Pipeline(steps = [\n",
    "    ('impOrd', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordtrasnf', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "\n",
    "# --- 3. Combinación de Pipelines con ColumnTransformer ---\n",
    "\n",
    "data_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Aplica la pipeline numérica a las columnas numéricas\n",
    "        ('num', numericas_pipe, numericas_pipe_nombres),\n",
    "        # Aplica la pipeline nominal a las columnas nominales\n",
    "        ('nom', nominales_pipe, nominales_pipe_nombres),\n",
    "        # Aplica la pipeline ordinal a las columnas ordinales\n",
    "        ('ord', ordinales_pipe, ordinales_pipe_nombres)\n",
    "    ],\n",
    "    remainder='passthrough', # Mantener columnas que no estén en la lista (si las hay)\n",
    "    verbose_feature_names_out=False # Simplifica los nombres de las features de salida\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "\n",
    "# --- 4. Función de Preprocesamiento Principal ---\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica el ColumnTransformer completo al DataFrame, realizando la imputación,\n",
    "    escalado y codificación necesarios para la implementación del modelo.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame con las variables a preprocesar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame transformado (o None si hay un error).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Iniciando Preprocesamiento de Datos ---\")\n",
    "\n",
    "    try:\n",
    "        # Ajusta y transforma los datos\n",
    "        # Nota: Si ya tienes un transformador ajustado (fit), usa solo .transform(df)\n",
    "        df_processed = data_preprocessor.fit_transform(df)\n",
    "\n",
    "        print(\"Preprocesamiento completado. Datos listos para el modelo.\")\n",
    "        print(f\"Shape de los datos transformados: {df_processed.shape}\")\n",
    "\n",
    "        return df_processed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante el preprocesamiento: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**División de la base entre entrenamiento y validación**\n",
    "\n",
    "* Variable objetivo (y): 'kredit'\n",
    "* El análisis muestra que hay un desbalanceo significativo entre las clases, pues la proporción de datos es 70:30, con lo cual podemos considerar que hay una diferencia considerable entre las clases de buenos y malos en el conjunto de datos.\n",
    "\n",
    "Éste desequilibrio puede afectar al rendimiento de los modelos de clasificación que podamos plantear, especialmente para el caso de la clase minoritaria. Para tratar de lidear con el problema, a lo largo del ejercicio se plantearán alternativas como el sobreajuste y subajuste, que nos apoyará a equilibrar el conjunto de datos, dándo mayor representación a la clase minoritatía y apoyando al desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kredit\n",
       "1.0    0.694556\n",
       "0.0    0.305444\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean['kredit'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_target(df: pd.DataFrame, target_column_name: str):\n",
    "    \"\"\"\n",
    "    Divide un DataFrame en dos partes:\n",
    "    X (variables predictoras/features) y y (variable objetivo/target).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame completo que contiene features y target.\n",
    "        target_column_name (str): El nombre de la columna que será la variable 'y'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Una tupla que contiene (X, y), donde X es un DataFrame y y es una Serie.\n",
    "               Devuelve (None, None) si la columna target no existe.\n",
    "    \"\"\"\n",
    "    # Verificar si la columna target existe en el DataFrame\n",
    "    if target_column_name not in df.columns:\n",
    "        print(f\"Error: La columna objetivo '{target_column_name}' no se encontró en el DataFrame.\")\n",
    "        return None, None\n",
    "\n",
    "    # Definir la variable objetivo (y)\n",
    "    y = df[target_column_name].copy()\n",
    "\n",
    "    # Definir las variables predictoras (X)\n",
    "    X = df.drop(columns=[target_column_name], axis=1).copy()\n",
    "\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, stratify= y, random_state=1234) # La semilla garantiza que nuestro código sea reproducible\n",
    "\n",
    "    print(f\"División de datos completada.\")\n",
    "    print(f\"   - X Train (Features): {Xtrain.shape[0]} filas, {Xtrain.shape[1]} columnas.\")\n",
    "    print(f\"   - X Test (Features): {Xtest.shape[0]} filas, {Xtest.shape[1]} columnas.\")\n",
    "    print(f\"   - y Train (Target '{target_column_name}'): {ytrain.shape[0]} filas.\")\n",
    "    print(f\"   - y Test (Target '{target_column_name}'): {ytest.shape[0]} filas.\")\n",
    "\n",
    "    return Xtrain, Xtest, ytrain, ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "División de datos completada.\n",
      "   - X Train (Features): 694 filas, 20 columnas.\n",
      "   - X Test (Features): 298 filas, 20 columnas.\n",
      "   - y Train (Target 'kredit'): 694 filas.\n",
      "   - y Test (Target 'kredit'): 298 filas.\n"
     ]
    }
   ],
   "source": [
    "## Ejecución del código para dividir entre train y test\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = split_data_by_target(data_clean, 'kredit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión de los datos de entrada:\n",
      "antes de aplicar las transformaciones: (694, 20)\n",
      "después de aplicar las transformaciones: (694, 123)\n"
     ]
    }
   ],
   "source": [
    "## Ejecutamos la transformación con el conjunto de entrenamiento\n",
    "\n",
    "Xtmp = Xtrain.copy()\n",
    "tmp = columnasTransformer.fit_transform(Xtmp)\n",
    "print(\"Dimensión de los datos de entrada:\")\n",
    "print(\"antes de aplicar las transformaciones:\", Xtmp.shape)\n",
    "\n",
    "print(\"después de aplicar las transformaciones:\", tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión de las variables de entrada ANTES de las transformaciones: (992, 20)\n",
      "Dimensión de las variables de entrada DESPUÉS de las transformaciones: (992, 154)\n"
     ]
    }
   ],
   "source": [
    "# Una vez ejecutada la transformación, y como se va a utilizar Validación-Cruzada, concatena los conjuntos de entrenamiento\n",
    "# y prueba en uno nuevo conjunto aumentado que llamaremos trainval:\n",
    "\n",
    "Xtraintest = pd.concat([Xtrain, Xtest], axis=0)\n",
    "ytraintest = pd.concat([ytrain, ytest], axis=0)\n",
    "\n",
    "# Veamos cuántas variables nuevas se introducen con las transformaciones One-Hot-Encoding:\n",
    "Xtmp = Xtraintest.copy()\n",
    "tmp = columnasTransformer.fit_transform(Xtmp)\n",
    "print(\"Dimensión de las variables de entrada ANTES de las transformaciones:\", Xtmp.shape)\n",
    "print(\"Dimensión de las variables de entrada DESPUÉS de las transformaciones:\", tmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenamiento de salida final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtraintest.to_csv(os.path.join('..', '..', 'data', 'processed', 'Xtraintest.csv'), index=False)\n",
    "ytraintest.to_csv(os.path.join('..', '..', 'data', 'processed', 'ytraintest.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
